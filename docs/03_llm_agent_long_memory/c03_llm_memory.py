import dashscope
import re
import json
import os
from dashscope import Generation
from typing import List, Dict, Optional

# ====================== å…¨å±€é…ç½® ======================
dashscope.api_key = "YOUR_API_KEY"
LONG_TERM_MEMORY_PATH = "llm_agent_long_memory.json"
LLM_MODEL = "qwen-turbo"
LLM_TEMPERATURE = 0.3

# ====================== é•¿æœŸè®°å¿†å·¥å…·å‡½æ•° ======================
def load_long_term_memory() -> List[Dict[str, str]]:
    if os.path.exists(LONG_TERM_MEMORY_PATH):
        try:
            with open(LONG_TERM_MEMORY_PATH, "r", encoding="utf-8") as f:
                memory = json.load(f)
                if isinstance(memory, list):
                    print(f"âœ… é•¿æœŸè®°å¿†åŠ è½½æˆåŠŸï¼Œå…±{len(memory)}æ¡å¯¹è¯è®°å½•")
                    return memory
                else:
                    print("âš ï¸ è®°å¿†æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåˆå§‹åŒ–ç©ºè®°å¿†")
                    return []
        except Exception as e:
            print(f"âš ï¸ åŠ è½½è®°å¿†å¼‚å¸¸ï¼š{str(e)}ï¼Œåˆå§‹åŒ–ç©ºè®°å¿†")
            return []
    else:
        with open(LONG_TERM_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)
        print("âœ… å·²åˆ›å»ºæ–°çš„é•¿æœŸè®°å¿†æ–‡ä»¶")
        return []

def save_long_term_memory(memory: List[Dict[str, str]], max_length: int = 20) -> None:
    try:
        trimmed_memory = memory[-max_length:]
        with open(LONG_TERM_MEMORY_PATH, "w", encoding="utf-8") as f:
            json.dump(trimmed_memory, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜é•¿æœŸè®°å¿†å¤±è´¥ï¼š{str(e)}")

# ====================== å·¥å…·å‡½æ•° ======================
def calculate_tool(expression: str) -> str:
    allowed_chars = set("0123456789+-*/(). ")
    if not all(char in allowed_chars for char in expression):
        return "è®¡ç®—é”™è¯¯ï¼šä»…æ”¯æŒæ•°å­—å’Œ+-*/()è¿ç®—"
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœï¼š{expression} = {result}"
    except ZeroDivisionError:
        return "è®¡ç®—é”™è¯¯ï¼šé™¤æ•°ä¸èƒ½ä¸º0"
    except SyntaxError:
        return "è®¡ç®—é”™è¯¯ï¼šè¡¨è¾¾å¼è¯­æ³•é”™è¯¯"
    except Exception:
        return "è®¡ç®—é”™è¯¯ï¼šæ— æ³•è¯†åˆ«çš„è¡¨è¾¾å¼æ ¼å¼"

def parse_tool_call(llm_output: str) -> Optional[Dict[str, str]]:
    pattern = r"TOOL:\s*calculate\((.*?)\)"
    match = re.search(pattern, llm_output.strip(), re.IGNORECASE)
    if match:
        return {"tool": "calculate", "params": match.group(1).strip()}
    return None

# ====================== æ™ºèƒ½ä½“ä¸»ç±» ======================
class LLMAgentWithLongMemory:
    def __init__(self):
        self.memory = load_long_term_memory()

    def perceive(self) -> str:
        user_input = input("ä½ ï¼š").strip()
        if not user_input:
            print("æ™ºèƒ½ä½“ï¼šè¯·è¾“å…¥æœ‰æ•ˆå†…å®¹ï½")
            return self.perceive()
        return user_input

    def decide(self, user_input: str) -> str:
        if "æ¸…ç©ºè®°å¿†" in user_input:
            self.memory = []
            save_long_term_memory(self.memory)
            return "âœ… å·²æ¸…ç©ºæ‰€æœ‰é•¿æœŸè®°å¿†ï¼"

        system_prompt = {
            "role": "system",
            "content": """
ä½ æ˜¯å…·å¤‡é•¿æœŸè®°å¿†å’Œå·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œéµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. è®°å¿†ï¼šä½ èƒ½è®°ä½æ‰€æœ‰å†å²å¯¹è¯ï¼Œé‡å¯åä¹Ÿä¸ä¼šä¸¢å¤±ï¼›
2. å·¥å…·ï¼šä»…æ‹¥æœ‰calculate(æ•°å­¦è¡¨è¾¾å¼)å·¥å…·ï¼Œè®¡ç®—éœ€æ±‚å¿…é¡»è¾“å‡ºï¼šTOOL: calculate(è¡¨è¾¾å¼)ï¼›
3. è¾“å‡ºï¼šå·¥å…·è°ƒç”¨ä»…è¿”å›æ ¼å¼æŒ‡ä»¤ï¼Œéè®¡ç®—éœ€æ±‚ç›´æ¥å‹å¥½å›å¤ï¼Œå›ç­”ç®€æ´ï¼ˆâ‰¤100å­—ï¼‰ã€‚
            """
        }

        messages = [system_prompt] + self.memory
        messages.append({"role": "user", "content": user_input})

        try:
            response = Generation.call(
                model=LLM_MODEL,
                messages=messages,
                temperature=LLM_TEMPERATURE,
                top_p=0.6
            )
            return response.output.text
        except Exception as e:
            return f"LLMè°ƒç”¨å¤±è´¥ï¼š{str(e)}"

    def act(self, llm_reply: str, user_input: str) -> str:
        tool_call = parse_tool_call(llm_reply)
        if tool_call and tool_call["tool"] == "calculate":
            final_reply = calculate_tool(tool_call["params"])
            print(f"æ™ºèƒ½ä½“ï¼ˆè®¡ç®—å™¨å·¥å…·ï¼‰ï¼š{final_reply}")
        else:
            final_reply = llm_reply
            print(f"æ™ºèƒ½ä½“ï¼š{final_reply}")

        self.memory.append({"role": "user", "content": user_input})
        self.memory.append({"role": "assistant", "content": final_reply})
        save_long_term_memory(self.memory)

        return final_reply

    def run(self) -> None:
        print("ğŸ“Œ LLMæ™ºèƒ½ä½“ï¼ˆå¸¦é•¿æœŸè®°å¿†ï¼‰å·²å¯åŠ¨ï¼Œè¾“å…¥'exit'é€€å‡ºå¯¹è¯\n")
        while True:
            user_input = self.perceive()
            if user_input.lower() == "exit":
                print("æ™ºèƒ½ä½“ï¼šå†è§ï¼å·²ä¿å­˜æ‰€æœ‰å¯¹è¯è®°å¿†ï½")
                break
            llm_reply = self.decide(user_input)
            self.act(llm_reply, user_input)

# ====================== è¿è¡Œå…¥å£ ======================
if __name__ == "__main__":
    agent = LLMAgentWithLongMemory()
    agent.run()